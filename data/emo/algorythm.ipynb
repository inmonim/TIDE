{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from kobert.utils import get_tokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1050\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/inmo/tide/data/emo/.cache/kobert_v1.zip\n",
      "using cached model. /home/inmo/tide/data/emo/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "kw_model = KeyBERT(bertmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module): ## 클래스를 상속\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=10,   ##클래스 수 조정##\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "max_len = 64   # 텍스트 데이터 최대 길이\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'models/'\n",
    "model = BERTClassifier(bertmodel)\n",
    "model.load_state_dict(torch.load(PATH + '10emotions_model_state_dict_2_10epoch.pt', map_location='cpu'))  # state_dict를 불러 온 후, 모델에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't redefine method: forward on class: __torch__.transformers.models.bert.modeling_bert.BertEmbeddings (of Python compilation unit at: 0x57608b0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_scripted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mscript(model)\n\u001b[1;32m      2\u001b[0m model_scripted\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39m./model_scripted.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tide/lib/python3.9/site-packages/torch/jit/_script.py:1257\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m   1256\u001b[0m     obj \u001b[39m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1257\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49mcreate_script_module(\n\u001b[1;32m   1258\u001b[0m         obj, torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49minfer_methods_to_compile\n\u001b[1;32m   1259\u001b[0m     )\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mdict\u001b[39m):\n\u001b[1;32m   1262\u001b[0m     \u001b[39mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[0;32m~/miniconda3/envs/tide/lib/python3.9/site-packages/torch/jit/_recursive.py:451\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    450\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[39m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 451\u001b[0m \u001b[39mreturn\u001b[39;00m create_script_module_impl(nn_module, concrete_type, stubs_fn)\n",
      "File \u001b[0;32m~/miniconda3/envs/tide/lib/python3.9/site-packages/torch/jit/_recursive.py:513\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    510\u001b[0m     script_module\u001b[39m.\u001b[39m_concrete_type \u001b[39m=\u001b[39m concrete_type\n\u001b[1;32m    512\u001b[0m \u001b[39m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mRecursiveScriptModule\u001b[39m.\u001b[39;49m_construct(cpp_module, init_fn)\n\u001b[1;32m    515\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/miniconda3/envs/tide/lib/python3.9/site-packages/torch/jit/_script.py:587\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[39mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[39mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[39m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    586\u001b[0m script_module \u001b[39m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 587\u001b[0m init_fn(script_module)\n\u001b[1;32m    589\u001b[0m \u001b[39m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[39m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    591\u001b[0m RecursiveScriptModule\u001b[39m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/miniconda3/envs/tide/lib/python3.9/site-packages/torch/jit/_recursive.py:491\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    488\u001b[0m     scripted \u001b[39m=\u001b[39m orig_value\n\u001b[1;32m    489\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     \u001b[39m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m     scripted \u001b[39m=\u001b[39m create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n\u001b[1;32m    493\u001b[0m cpp_module\u001b[39m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    494\u001b[0m script_module\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m scripted\n",
      "File \u001b[0;32m~/miniconda3/envs/tide/lib/python3.9/site-packages/torch/jit/_recursive.py:513\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    510\u001b[0m     script_module\u001b[39m.\u001b[39m_concrete_type \u001b[39m=\u001b[39m concrete_type\n\u001b[1;32m    512\u001b[0m \u001b[39m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mRecursiveScriptModule\u001b[39m.\u001b[39;49m_construct(cpp_module, init_fn)\n\u001b[1;32m    515\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m~/miniconda3/envs/tide/lib/python3.9/site-packages/torch/jit/_script.py:587\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[39mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[39mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[39m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    586\u001b[0m script_module \u001b[39m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 587\u001b[0m init_fn(script_module)\n\u001b[1;32m    589\u001b[0m \u001b[39m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    590\u001b[0m \u001b[39m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    591\u001b[0m RecursiveScriptModule\u001b[39m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m~/miniconda3/envs/tide/lib/python3.9/site-packages/torch/jit/_recursive.py:491\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    488\u001b[0m     scripted \u001b[39m=\u001b[39m orig_value\n\u001b[1;32m    489\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     \u001b[39m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m     scripted \u001b[39m=\u001b[39m create_script_module_impl(orig_value, sub_concrete_type, stubs_fn)\n\u001b[1;32m    493\u001b[0m cpp_module\u001b[39m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    494\u001b[0m script_module\u001b[39m.\u001b[39m_modules[name] \u001b[39m=\u001b[39m scripted\n",
      "File \u001b[0;32m~/miniconda3/envs/tide/lib/python3.9/site-packages/torch/jit/_recursive.py:517\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[39mif\u001b[39;00m concrete_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m concrete_type_store\u001b[39m.\u001b[39mmethods_compiled:\n\u001b[0;32m--> 517\u001b[0m     create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)\n\u001b[1;32m    518\u001b[0m     \u001b[39m# Create hooks after methods to ensure no name collisions between hooks and methods.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m     \u001b[39m# If done before, hooks can overshadow methods that aren't exported.\u001b[39;00m\n\u001b[1;32m    520\u001b[0m     create_hooks_from_stubs(concrete_type, hook_stubs, pre_hook_stubs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tide/lib/python3.9/site-packages/torch/jit/_recursive.py:368\u001b[0m, in \u001b[0;36mcreate_methods_and_properties_from_stubs\u001b[0;34m(concrete_type, method_stubs, property_stubs)\u001b[0m\n\u001b[1;32m    365\u001b[0m property_defs \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mdef_ \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m property_stubs]\n\u001b[1;32m    366\u001b[0m property_rcbs \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mresolution_callback \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m property_stubs]\n\u001b[0;32m--> 368\u001b[0m concrete_type\u001b[39m.\u001b[39;49m_create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't redefine method: forward on class: __torch__.transformers.models.bert.modeling_bert.BertEmbeddings (of Python compilation unit at: 0x57608b0)"
     ]
    }
   ],
   "source": [
    "model_scripted = torch.jit.script(model)\n",
    "model_scripted.save('./model_scripted.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /home/inmo/tide_pjt/data/emo/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(predict_sentence):\n",
    "\n",
    "    data = [predict_sentence, '0']\n",
    "    dataset_another = [data]\n",
    "\n",
    "    another_test = BERTDataset(dataset_another, 0, 1, tok, max_len, True, False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=2)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for (token_ids, valid_length, segment_ids, label) in test_dataloader:\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "\n",
    "        # for i, e in zip(out[0], emotion_list):\n",
    "        #     print(f'{e}: {round(float(i),4)}')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emo_rank(pre_result):\n",
    "    if pre_result[8] >= math.sqrt(sum(abs(pre_result))/10):\n",
    "        pre_result[8] = max(float(pre_result[8] ** 2), pre_result[8])\n",
    "    if pre_result[9] >= math.sqrt(sum(abs(pre_result))/10):\n",
    "        pre_result[9] = max(float(pre_result[9] ** 2), pre_result[9])\n",
    "        \n",
    "    for i in range(8):\n",
    "        pre_result[i] = (pre_result[i]/5)*4\n",
    "    \n",
    "    mask = sorted(enumerate(pre_result), key=lambda x:x[1], reverse=True)\n",
    "    e = []\n",
    "    tmp = []\n",
    "    for i, x in mask:\n",
    "        if len(e) >= 3:\n",
    "            break\n",
    "        if x > float(sum(abs(pre_result))/len(pre_result)):\n",
    "            e.append(i+1)\n",
    "        elif x >= math.sqrt(abs(sum(pre_result)))/len(pre_result):\n",
    "            tmp.append(i+11)\n",
    "    else:\n",
    "        while len(e) < 3:\n",
    "            if tmp:\n",
    "                e.append(tmp.pop(0))\n",
    "            else:\n",
    "                e.append(0)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3929,  0.0689, -0.1116,  0.7096,  0.7330,  2.0293, -0.7949, -0.8528,\n",
       "         -2.4520, -1.7598]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''아 프로젝트 너무 힘들다. 개빡친다! 자고 싶다! 너무 힘들다!'''\n",
    "emotion = predict(text)\n",
    "emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('자고', 0.733), ('너무', 0.5021), ('힘들다', 0.4998)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=3)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 10, 0]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_rank(emotion[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./song_emotion_keyword.csv', index_col=0)\n",
    "song = pd.read_csv('../song/song_data/song.csv', encoding='cp949',  index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "keyword_list = [i[0] for i in keywords]\n",
    "e1, e2, e3 = emo_rank(emotion[0])\n",
    "song_dic = {}\n",
    "\n",
    "def add_score(i, score, is_sec):\n",
    "    if song_dic.get(i):\n",
    "        song_dic[i] += (score+random.random())/is_sec\n",
    "    else:\n",
    "        song_dic[i] = score/is_sec\n",
    "    \n",
    "\n",
    "is_sec = 2 if e1 > 10 else 1\n",
    "x = df[df.loc[:, 'emotion_1'] == e1]\n",
    "for i in x.song_id:\n",
    "    rs = round(random.random(),3)\n",
    "    song_dic[i] = (100+rs)/is_sec\n",
    "x = df[df.loc[:, 'emotion_2'] == e1]\n",
    "for i in x.song_id:\n",
    "    rs = round(random.random(),3)\n",
    "    song_dic[i] = (80+rs)/is_sec\n",
    "x = df[df.loc[:, 'emotion_3'] == e1]\n",
    "for i in x.song_id:\n",
    "    rs = round(random.random(),3)\n",
    "    song_dic[i] = (50+rs)/is_sec\n",
    "\n",
    "is_sec = 3 if e2 > 10 else 1\n",
    "x = df[df.loc[:, 'emotion_1'] == e2]\n",
    "for i in x.song_id:\n",
    "    add_score(i, 70, is_sec)\n",
    "x = df[df.loc[:, 'emotion_2'] == e2]\n",
    "for i in x.song_id:\n",
    "    add_score(i, 90, is_sec)\n",
    "x = df[df.loc[:, 'emotion_3'] == e2]\n",
    "for i in x.song_id:\n",
    "    add_score(i, 50, is_sec)\n",
    "\n",
    "is_sec = 3 if e3 > 10 else 1\n",
    "x = df[df.loc[:, 'emotion_1'] == e3]\n",
    "for i in x.song_id:\n",
    "    add_score(i, 40, is_sec)\n",
    "x = df[df.loc[:, 'emotion_2'] == e3]\n",
    "for i in x.song_id:\n",
    "    add_score(i, 50, is_sec)\n",
    "x = df[df.loc[:, 'emotion_3'] == e3]\n",
    "for i in x.song_id:\n",
    "    add_score(i, 60, is_sec)\n",
    "    \n",
    "for word in keyword_list:\n",
    "    x = df[df.loc[:, 'key_sentence'].str.contains(word)]\n",
    "    for i in x.song_id:\n",
    "        add_score(i, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[56251, 302.97320243830063],\n",
       " [8204539, 262.80269885445523],\n",
       " [78853, 252.78768658161476],\n",
       " [3549031, 252.24612169124487],\n",
       " [4635701, 252.03869593643628],\n",
       " [3573326, 251.84312789662755],\n",
       " [31266290, 251.7537439968295],\n",
       " [2654948, 251.63220805897603],\n",
       " [891991, 251.60079168872485],\n",
       " [5644826, 251.54448350750968]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_dic\n",
    "rec_list = sorted([[k,v]for k, v in song_dic.items()],key=lambda x: x[1], reverse=True)\n",
    "rec_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.104"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "584       607928\n",
       "1251       85508\n",
       "1849      316664\n",
       "2193      621812\n",
       "2595      928705\n",
       "2931     1314788\n",
       "3811     1898142\n",
       "3907     1932254\n",
       "4285     2247067\n",
       "5054     3164052\n",
       "5681     3764815\n",
       "6013     3978258\n",
       "6066     4001753\n",
       "6216     4100521\n",
       "6935     5450271\n",
       "7085     5683227\n",
       "7869    30657307\n",
       "8144    31331750\n",
       "8202    31455159\n",
       "8521    32399832\n",
       "8544     3118484\n",
       "Name: song_id, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.loc[:, 'key_sentence'].str.contains('오늘도')].song_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fds\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "string = \"aHello, World!\"\n",
    "pattern = \"Hello\"\n",
    "\n",
    "if re.search(pattern, string):\n",
    "    print('fds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

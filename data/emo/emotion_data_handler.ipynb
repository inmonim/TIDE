{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_df = pd.read_excel('song_df.xlsx', index_col=0)\n",
    "lyrics_df = pd.read_excel('song_lyrics_df.xlsx', index_col=0)\n",
    "song_album_df = pd.read_excel('song_album_df.xlsx', index_col=0)\n",
    "song_artist_df = pd.read_excel('song_artist_df.xlsx', index_col=0)\n",
    "song_genre_df = pd.read_excel('song_genre_df.xlsx', index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19금 노래 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for st in song_df['title']:\n",
    "    if st[:3] == '19금':\n",
    "        song_df.drop(song_df[song_df['title'] == st].index, inplace=True)\n",
    "\n",
    "song_df = song_df.drop_duplicates('song_id')\n",
    "song_df.reset_index(drop=True)\n",
    "song_df.to_excel('./song_df.xlsx')\n",
    "\n",
    "\n",
    "def drop_none_song(song_df, df):\n",
    "    for si in df['song_id']:\n",
    "        if song_df[song_df['song_id'] == si].empty:\n",
    "            df.drop(df[df['song_id'] == si].index, inplace=True)\n",
    "\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_album_df = drop_none_song(song_df, song_album_df)\n",
    "song_artist_df = drop_none_song(song_df, song_artist_df)\n",
    "song_genre_df = drop_none_song(song_df, song_genre_df)\n",
    "lyrics_df = drop_none_song(song_df, lyrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_album_df.to_excel('./song_album_df.xlsx')\n",
    "song_artist_df.to_excel('./song_artist_df.xlsx')\n",
    "song_genre_df.to_excel('./song_genre_df.xlsx')\n",
    "lyrics_df.to_excel('./song_lyrics_df.xlsx')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자연어 처리용 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lyrics_df)):\n",
    "    song_id = lyrics_df.loc[i, 'song_id']\n",
    "    lyrics_df.loc[i, 'title'] = song_df[song_df['song_id'] == song_id].title.item()\n",
    "\n",
    "lyrics_df.to_excel('lyrics_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "lyrics_df = pd.read_excel('lyrics_data.xlsx',index_col=0)\n",
    "\n",
    "lyrics = lyrics_df.lyrics[8000].replace(' \\\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "\n",
    "lyrics_word = okt.morphs(lyrics, norm=True)\n",
    "\n",
    "stopword = ['도', '는', '다', '의', '가', '이', '은', ',', '될',\n",
    "            '한', '에', '하', '고', '을', '를', '인', '.', '로',\n",
    "            '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\n",
    "\n",
    "word_token = [w for w in lyrics_word if not w in stopword]\n",
    "\n",
    "word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 감성대화"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원천 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./018.감성대화/Training_221115_add/라벨링데이터/감성대화말뭉치(최종데이터)_Training/감성대화말뭉치(최종데이터)_Training.json', encoding='utf-8') as json_f:\n",
    "    json_sen = json.load(json_f)\n",
    "\n",
    "with open('./018.감성대화/Validation_221115_add/라벨링데이터/감성대화말뭉치(최종데이터)_Validation/감성대화말뭉치(최종데이터)_Validation.json', encoding='utf-8') as json_t_f:\n",
    "    json_t_sen = json.load(json_t_f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 새 데이터 프레임 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_df = pd.DataFrame(columns=['sentense','emotion'])\n",
    "emo_test_df = pd.DataFrame(columns=['sentense', 'emotion'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF에 데이터 인풋 및 CSV 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in json_sen:\n",
    "    emo = sen['profile']['emotion']['type']\n",
    "    conv = sen['talk']['content']\n",
    "    cnt = 0\n",
    "    for t in conv:\n",
    "        if t[0] == 'H' and conv[t]:\n",
    "            emo_df.loc[len(emo_df),:] = [conv[t], emo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_df.to_csv('./emo_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen in notebook.tqdm(json_t_sen):\n",
    "    emo = sen['profile']['emotion']['type']\n",
    "    conv = sen['talk']['content']\n",
    "    for t in conv:\n",
    "        if t[0] == 'H' and conv[t]:\n",
    "            emo_test_df.loc[len(emo_test_df),:] = [conv[t], emo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_test_df.to_csv('./emo_test_df.csv', index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 감성 정리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VER_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "emo_df = pd.read_csv('emo_data/emo_df.csv', index_col=0)\n",
    "emo_test_df = pd.read_csv('emo_data/emo_test_df.csv', index_col=0)\n",
    "\n",
    "분노 = (10, 11, 12, 13, 14, 16)\n",
    "악의 = (15, 17, 18, 19, 57, 58)\n",
    "슬픔 = (20, 21, 23, 24, 53, 51)\n",
    "절망 = (22, 25, 26, 27, 28, 29)\n",
    "당황 = (32, 34, 35, 38, 50, 59)\n",
    "불안 = (30, 31, 33, 36, 37, 39)\n",
    "열등 = (41, 43, 45, 46, 47, 54, 52)\n",
    "상처 = (40, 42, 44, 48, 49, 55, 56)\n",
    "사랑 = (60, 61, 62, 64, 65)\n",
    "편안 = (63, 66, 67, 68, 69)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VER_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "분노 = (10, 11, 12, 13, 14, 16)\n",
    "악의 = (15, 17, 18, 57, 58)\n",
    "슬픔 = (20, 21, 23, 24, 53, 51, 27)\n",
    "절망 = (22, 25, 26, 28, 29)\n",
    "당황 = (32, 34, 35, 38, 50, 59)\n",
    "불안 = (30, 31, 33, 36, 37, 39, 19)\n",
    "열등 = (41, 43, 45, 46, 47, 54, 52)\n",
    "상처 = (40, 42, 44, 48, 49, 55, 56)\n",
    "사랑 = (60, 61, 62, 64, 65)\n",
    "편안 = (63, 66, 67, 68, 69)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VER_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "분노 = (10, 11, 12, 13, 14, 16)\n",
    "악의 = (15, 17, 18, 57, 58)\n",
    "슬픔 = (20, 21, 23, 24, 53, 51, 27)\n",
    "절망 = (22, 25, 26, 28, 29)\n",
    "당황 = (32, 34, 35, 38, 50, 59)\n",
    "불안 = (30, 31, 33, 36, 37, 39, 19)\n",
    "열등 = (41, 43, 45, 46, 47, 54, 52)\n",
    "상처 = (40, 42, 44, 48, 49, 55, 56)\n",
    "사랑 = (60, 61, 62, 64, 65)\n",
    "편안 = (63, 66, 67, 68, 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5ed340640145d7863305a58b64f27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145959 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce87247c73748ab83fdc370ad7f4613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17965 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def emotion_orz(df, name):\n",
    "    for i in tqdm(range(len(df))):\n",
    "        e = int(df['emotion'][i][1:])\n",
    "        if e in 분노:\n",
    "            e = '0'\n",
    "        elif e in 악의:\n",
    "            e = '1'\n",
    "        elif e in 슬픔:\n",
    "            e = '2'\n",
    "        elif e in 절망:\n",
    "            e = '3'\n",
    "        elif e in 당황:\n",
    "            e = '4'\n",
    "        elif e in 불안:\n",
    "            e = '5'\n",
    "        elif e in 열등:\n",
    "            e = '6'\n",
    "        elif e in 상처:\n",
    "            e = '7'\n",
    "        elif e in 사랑:\n",
    "            e = '8'\n",
    "        elif e in 편안:\n",
    "            e = '9'\n",
    "        \n",
    "        df.loc[i, 'emotion'] = e\n",
    "\n",
    "    df.to_csv(f'{name}_newemo.csv')\n",
    "\n",
    "emotion_orz(emo_df, 'emo_df')\n",
    "emotion_orz(emo_test_df, 'emo_test_df')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이저?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cf362d43ac47b3b7ba51951fc34e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145959 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m test_data \u001b[39m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m notebook\u001b[39m.\u001b[39mtqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(emo_df))):\n\u001b[1;32m----> 5\u001b[0m     train_data\u001b[39m.\u001b[39mappend([emo_df\u001b[39m.\u001b[39;49mloc[i, \u001b[39m'\u001b[39;49m\u001b[39msentense\u001b[39;49m\u001b[39m'\u001b[39;49m], emo_df\u001b[39m.\u001b[39mloc[i, \u001b[39m'\u001b[39m\u001b[39memotion\u001b[39m\u001b[39m'\u001b[39m]])\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m notebook\u001b[39m.\u001b[39mtqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(emo_test_df))):\n\u001b[0;32m      8\u001b[0m     test_data\u001b[39m.\u001b[39mappend([emo_test_df\u001b[39m.\u001b[39mloc[i, \u001b[39m'\u001b[39m\u001b[39msentense\u001b[39m\u001b[39m'\u001b[39m], emo_df\u001b[39m.\u001b[39mloc[i, \u001b[39m'\u001b[39m\u001b[39memotion\u001b[39m\u001b[39m'\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1063\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1061\u001b[0m check_deprecated_indexers(key)\n\u001b[0;32m   1062\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(key) \u001b[39mis\u001b[39;00m \u001b[39mtuple\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39;49m(\u001b[39mlist\u001b[39;49m(x) \u001b[39mif\u001b[39;49;00m is_iterator(x) \u001b[39melse\u001b[39;49;00m x \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m key)\n\u001b[0;32m   1064\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(com\u001b[39m.\u001b[39mapply_if_callable(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[0;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\miniconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1063\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1061\u001b[0m check_deprecated_indexers(key)\n\u001b[0;32m   1062\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(key) \u001b[39mis\u001b[39;00m \u001b[39mtuple\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mlist\u001b[39m(x) \u001b[39mif\u001b[39;00m is_iterator(x) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[0;32m   1064\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(com\u001b[39m.\u001b[39mapply_if_callable(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[0;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for i in notebook.tqdm(range(len(emo_df))):\n",
    "    train_data.append([emo_df.loc[i, 'sentense'], emo_df.loc[i, 'emotion']])\n",
    "\n",
    "for i in notebook.tqdm(range(len(emo_test_df))):\n",
    "    test_data.append([emo_test_df.loc[i, 'sentense'], emo_df.loc[i, 'emotion']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여긴 나중에"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 제거 및 트레인 셋으로 워드백 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train = []\n",
    "t_test = []\n",
    "\n",
    "stopword = ['도', '는', '다', '의', '가', '이', '은', ',', '될',\n",
    "            '한', '에', '하', '고', '을', '를', '인', '.', '로',\n",
    "            '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\n",
    "\n",
    "for sentense in notebook.tqdm(emo_df['sentense']):\n",
    "    word_back = okt.morphs(sentense, norm=True)\n",
    "    word_back = [word for word in word_back if not word in stopword]\n",
    "    t_train.append(word_back)\n",
    "\n",
    "for sentense in notebook.tqdm(emo_test_df['sentense']):\n",
    "    word_back = okt.morphs(sentense, norm=True)\n",
    "    word_back = [word for word in word_back if not word in stopword]\n",
    "    t_test.append(word_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_object = []\n",
    "for i in notebook.tqdm(range(len(t_train))):\n",
    "    tmp = {}\n",
    "    tmp = {'sentense' : t_train[i], 'emotion' : emo_df.iloc[i, 1]}\n",
    "    json_object.append(tmp)\n",
    "\n",
    "with open('./sentense_train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_object, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_test_object = []\n",
    "for i in notebook.tqdm(range(len(t_test))):\n",
    "    tmp = {}\n",
    "    tmp = {'sentense' : t_test[i], 'emotion' : emo_test_df.iloc[i, 1]}\n",
    "    json_test_object.append(tmp)\n",
    "\n",
    "with open('./sentense_test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_test_object, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./sentense_train.json', 'r', encoding='utf-8') as train_f:\n",
    "    t_train_json = json.load(train_f)\n",
    "    \n",
    "with open('./sentense_test.json', 'r', encoding='utf-8') as test_f:\n",
    "    t_test_json = json.load(test_f)\n",
    "\n",
    "\n",
    "t_train = []\n",
    "for sen in t_train_json:\n",
    "    t_train.append(sen['sentense'])\n",
    "    \n",
    "t_test = []\n",
    "for sen in t_test_json:\n",
    "    t_test.append(sen['sentense'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt) * 100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train1 = tokenizer.texts_to_sequences(t_train)\n",
    "\n",
    "t_test1 = tokenizer.texts_to_sequences(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d3f5ee38ea4182a6d3ed6fd5544926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in tqdm(range(10000000)):\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('emo_df.csv', index_col=0)\n",
    "test_df = pd.read_csv('emo_test_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f09ff0d3a5485aa04349fbf3e20606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145959 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9742a297bf6946be9d4e83d6c827ba64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17965 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(len(train_df))):\n",
    "    train_df.loc[i, 'emotion'] = int(train_df.loc[i, 'emotion'][1:2])-1\n",
    "\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    test_df.loc[i, 'emotion'] = int(test_df.loc[i, 'emotion'][1:2])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('emo_df_6emo.csv')\n",
    "test_df.to_csv('emo_df_test_6emo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('emo_df_6emo.csv', index_col=0)\n",
    "test_df = pd.read_csv('emo_test_df_6emo.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 5, 2, 4, 1, 3], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.emotion.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
